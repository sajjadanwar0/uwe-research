{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpK2hcGl0OJKClCI79PHGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajjadanwar0/uwe-research/blob/master/notebooks/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9NuFxrMQMeP"
      },
      "outputs": [],
      "source": [
        "__all__ = ['show_image', 'subplots', 'get_grid', 'show_images', 'init_ddpm', 'load_image', 'image_grid', 'plot_scheduler',\n",
        "           'plot_noise_and_denoise', 'spectrogram_from_image', 'waveform_from_spectrogram',\n",
        "           'wav_bytes_from_spectrogram_image', 'measure_latency_and_memory_use']\n",
        "\n",
        "\n",
        " import gc\n",
        "import math\n",
        "import typing\n",
        "from io import BytesIO\n",
        "from itertools import zip_longest\n",
        "\n",
        "import fastcore.all as fc\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchvision.transforms.functional as TF\n",
        "from matplotlib import pyplot as plt\n",
        "from nbdev.showdoc import *\n",
        "from PIL import Image\n",
        "from scipy.io import wavfile\n",
        "from torch.nn import init\n",
        "from torchvision.utils import make_grid\n",
        "from transformers import set_seed\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@fc.delegates(plt.Axes.imshow)\n",
        "def show_image(im, ax=None, figsize=None, title=None, noframe=True, **kwargs):\n",
        "    \"Show a PIL or PyTorch image on `ax`.\"\n",
        "    if fc.hasattrs(im, (\"cpu\", \"permute\", \"detach\")):\n",
        "        im = im.detach().cpu()\n",
        "        if len(im.shape) == 3 and im.shape[0] < 5:\n",
        "            im = im.permute(1, 2, 0)\n",
        "    elif not isinstance(im, np.ndarray):\n",
        "        im = np.array(im)\n",
        "    if im.shape[-1] == 1:\n",
        "        im = im[..., 0]\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(figsize=figsize)\n",
        "    ax.imshow(im, **kwargs)\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    if noframe:\n",
        "        ax.axis(\"off\")\n",
        "    return ax\n",
        "\n",
        "\n",
        "@fc.delegates(plt.subplots, keep=True)\n",
        "def subplots(\n",
        "    nrows: int = 1,  # Number of rows in returned axes grid\n",
        "    ncols: int = 1,  # Number of columns in returned axes grid\n",
        "    figsize: tuple = None,  # Width, height in inches of the returned figure\n",
        "    imsize: int = 3,  # Size (in inches) of images that will be displayed in the returned figure\n",
        "    suptitle: str = None,  # Title to be set to returned figure\n",
        "    **kwargs,\n",
        "):  # fig and axs\n",
        "    \"A figure and set of subplots to display images of `imsize` inches\"\n",
        "    if figsize is None:\n",
        "        figsize = (ncols * imsize, nrows * imsize)\n",
        "    fig, ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n",
        "    if suptitle is not None:\n",
        "        fig.suptitle(suptitle)\n",
        "    if nrows * ncols == 1:\n",
        "        ax = np.array([ax])\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "@fc.delegates(subplots)\n",
        "def get_grid(\n",
        "    n: int,  # Number of axes\n",
        "    nrows: int = None,  # Number of rows, defaulting to `int(math.sqrt(n))`\n",
        "    ncols: int = None,  # Number of columns, defaulting to `ceil(n/rows)`\n",
        "    title: str = None,  # If passed, title set to the figure\n",
        "    weight: str = \"bold\",  # Title font weight\n",
        "    size: int = 14,  # Title font size\n",
        "    **kwargs,\n",
        "):  # fig and axs\n",
        "    \"Return a grid of `n` axes, `rows` by `cols`\"\n",
        "    if nrows:\n",
        "        ncols = ncols or int(np.floor(n / nrows))\n",
        "    elif ncols:\n",
        "        nrows = nrows or int(np.ceil(n / ncols))\n",
        "    else:\n",
        "        nrows = int(math.sqrt(n))\n",
        "        ncols = int(np.floor(n / nrows))\n",
        "    fig, axs = subplots(nrows, ncols, **kwargs)\n",
        "    for i in range(n, nrows * ncols):\n",
        "        axs.flat[i].set_axis_off()\n",
        "    if title is not None:\n",
        "        fig.suptitle(title, weight=weight, size=size)\n",
        "    return fig, axs\n",
        "\n",
        "\n",
        "@fc.delegates(subplots)\n",
        "def show_images(\n",
        "    ims: list,  # Images to show\n",
        "    nrows: typing.Union[int, None] = None,  # Number of rows in grid\n",
        "    ncols: typing.Union[\n",
        "        int, None\n",
        "    ] = None,  # Number of columns in grid (auto-calculated if None)\n",
        "    titles: typing.Union[\n",
        "        list, None\n",
        "    ] = None,  # Optional list of titles for each image\n",
        "    **kwargs,\n",
        "):\n",
        "    \"Show all images `ims` as subplots with `rows` using `titles`\"\n",
        "    axs = get_grid(len(ims), nrows, ncols, **kwargs)[1].flat\n",
        "    for im, t, ax in zip_longest(ims, titles or [], axs):\n",
        "        show_image(im, ax=ax, title=t)\n",
        "\n",
        "def init_ddpm(model):\n",
        "    for o in model.down_blocks:\n",
        "        for p in o.resnets:\n",
        "            p.conv2.weight.data.zero_()\n",
        "            for p in fc.L(o.downsamplers):\n",
        "                init.orthogonal_(p.conv.weight)\n",
        "\n",
        "    for o in model.up_blocks:\n",
        "        for p in o.resnets:\n",
        "            p.conv2.weight.data.zero_()\n",
        "\n",
        "    model.conv_out.weight.data.zero_()\n",
        "\n",
        "def load_image(url, size=None, return_tensor=False):\n",
        "    if url.startswith(\"http\"):\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "    else:\n",
        "        img = Image.open(url)\n",
        "    if size is not None:\n",
        "        img = img.resize(size)\n",
        "    if return_tensor:\n",
        "        return TF.to_tensor(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "\n",
        "\n",
        "def plot_scheduler(scheduler, ax=None, plot_both=True, label=None):\n",
        "    if ax is None:\n",
        "        fig, (ax) = plt.subplots(1, 1)\n",
        "    # Check if SimpleScheduler\n",
        "    if not hasattr(scheduler, \"alphas_cumprod\"):\n",
        "        ax.plot(\n",
        "            torch.linspace(1, 0, scheduler.num_train_timesteps),\n",
        "            label=r\"${\\sqrt{\\bar{\\alpha}_t}}$ equivalent\",\n",
        "        )\n",
        "        if plot_both:\n",
        "            ax.plot(\n",
        "                torch.linspace(0, 1, scheduler.num_train_timesteps),\n",
        "                label=r\"$\\sqrt{(1 - \\bar{\\alpha}_t)}$ equivalent\",\n",
        "            )\n",
        "        ax.legend()\n",
        "        return\n",
        "    if label == None:\n",
        "        label = r\"${\\sqrt{\\bar{\\alpha}_t}}$\"\n",
        "    ax.plot(scheduler.alphas_cumprod.cpu() ** 0.5, label=label)\n",
        "    if plot_both:\n",
        "        ax.plot(\n",
        "            (1 - scheduler.alphas_cumprod.cpu()) ** 0.5,\n",
        "            label=r\"$\\sqrt{(1 - \\bar{\\alpha}_t)}$\",\n",
        "        )\n",
        "    ax.legend(fontsize=\"x-large\");\n",
        "\n",
        "def plot_noise_and_denoise(scheduler_output, step):\n",
        "    _, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    prev_prev_sample = scheduler_output.prev_sample\n",
        "    grid = make_grid(prev_prev_sample, nrow=4).permute(1, 2, 0)\n",
        "    axs[0].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n",
        "    axs[0].set_title(f\"Current x (step {step})\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    pred_x0 = scheduler_output.pred_original_sample\n",
        "    grid = make_grid(pred_x0, nrow=4).permute(1, 2, 0)\n",
        "    axs[1].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n",
        "    axs[1].set_title(f\"Predicted denoised images (step {step})\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\"\"\"\n",
        "Simplified from riffusion codebase.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def spectrogram_from_image(image, max_volume, power_for_image) -> np.ndarray:\n",
        "    # Convert to a numpy array of floats\n",
        "    data = np.array(image).astype(np.float32)\n",
        "\n",
        "    # Flip vertically and take a single channel\n",
        "    data = data[::-1, :, 0]\n",
        "\n",
        "    # Invert\n",
        "    data = 255 - data\n",
        "\n",
        "    # Rescale to max volume\n",
        "    data = data * max_volume / 255\n",
        "\n",
        "    # Reverse the power curve\n",
        "    data = np.power(data, 1 / power_for_image)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def waveform_from_spectrogram(\n",
        "    Sxx: np.ndarray,\n",
        "    n_fft: int,\n",
        "    hop_length: int,\n",
        "    win_length: int,\n",
        "    num_samples: int,\n",
        "    sample_rate: int,\n",
        "    n_mels: int,\n",
        "    max_mel_iters: int,\n",
        "    num_griffin_lim_iters: int,\n",
        "    device: str = \"cpu\",\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Reconstruct a waveform from a spectrogram.\n",
        "    This is an approximate inverse of spectrogram_from_waveform, using the Griffin-Lim algorithm\n",
        "    to approximate the phase.\n",
        "    \"\"\"\n",
        "    Sxx_torch = torch.from_numpy(Sxx).to(device)\n",
        "\n",
        "    mel_inv_scaler = torchaudio.transforms.InverseMelScale(\n",
        "        n_mels=n_mels,\n",
        "        sample_rate=sample_rate,\n",
        "        f_min=0,\n",
        "        f_max=10000,\n",
        "        n_stft=n_fft // 2 + 1,\n",
        "        norm=None,\n",
        "        mel_scale=\"htk\",\n",
        "        max_iter=max_mel_iters,\n",
        "    ).to(device)\n",
        "\n",
        "    Sxx_torch = mel_inv_scaler(Sxx_torch)\n",
        "\n",
        "    griffin_lim = torchaudio.transforms.GriffinLim(\n",
        "        n_fft=n_fft,\n",
        "        win_length=win_length,\n",
        "        hop_length=hop_length,\n",
        "        power=1.0,\n",
        "        n_iter=num_griffin_lim_iters,\n",
        "    ).to(device)\n",
        "\n",
        "    waveform = griffin_lim(Sxx_torch).cpu().numpy()\n",
        "\n",
        "    return waveform\n",
        "\n",
        "\n",
        "def wav_bytes_from_spectrogram_image(image, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Reconstruct a WAV audio clip from a spectrogram image. Also returns the duration in seconds.\n",
        "    \"\"\"\n",
        "\n",
        "    max_volume = 50\n",
        "    power_for_image = 0.25\n",
        "    Sxx = spectrogram_from_image(\n",
        "        image, max_volume=max_volume, power_for_image=power_for_image\n",
        "    )\n",
        "\n",
        "    sample_rate = 44100  # [Hz]\n",
        "    clip_duration_ms = 5000  # [ms]\n",
        "\n",
        "    bins_per_image = 512\n",
        "    n_mels = 512\n",
        "\n",
        "    # FFT parameters\n",
        "    window_duration_ms = 100  # [ms]\n",
        "    padded_duration_ms = 400  # [ms]\n",
        "    step_size_ms = 10  # [ms]\n",
        "\n",
        "    # Derived parameters\n",
        "    num_samples = (\n",
        "        int(image.width / float(bins_per_image) * clip_duration_ms)\n",
        "        * sample_rate\n",
        "    )\n",
        "    n_fft = int(padded_duration_ms / 1000.0 * sample_rate)\n",
        "    hop_length = int(step_size_ms / 1000.0 * sample_rate)\n",
        "    win_length = int(window_duration_ms / 1000.0 * sample_rate)\n",
        "\n",
        "    samples = waveform_from_spectrogram(\n",
        "        Sxx=Sxx,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        win_length=win_length,\n",
        "        num_samples=num_samples,\n",
        "        sample_rate=sample_rate,\n",
        "        n_mels=n_mels,\n",
        "        max_mel_iters=200,\n",
        "        num_griffin_lim_iters=32,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    wav_bytes = BytesIO()\n",
        "    wavfile.write(wav_bytes, sample_rate, samples.astype(np.int16))\n",
        "    wav_bytes.seek(0)\n",
        "\n",
        "    return wav_bytes\n",
        "\n",
        "\n",
        "\n",
        "def measure_latency_and_memory_use(\n",
        "    pipeline, inputs, model_name, device, nb_loops=50\n",
        "):\n",
        "    # Define Events that measure start and end of the generate pass\n",
        "    start_event = torch.cuda.Event(enable_timing=True)\n",
        "    end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    # Reset cuda memory stats and empty cache\n",
        "    torch.cuda.reset_peak_memory_stats(device)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    gc.collect()  # Needed due to Ubuntu\n",
        "\n",
        "    # Get the start time\n",
        "    start_event.record()\n",
        "\n",
        "    # Perform generation\n",
        "    for _ in range(nb_loops):\n",
        "        set_seed(0)\n",
        "        _ = pipeline(inputs)\n",
        "\n",
        "    # Get end time\n",
        "    end_event.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Measure memory footprint and elapsed time\n",
        "    max_memory = torch.cuda.max_memory_allocated(device)\n",
        "    elapsed_time = start_event.elapsed_time(end_event) * 1.0e-3\n",
        "\n",
        "    print(f\"{model_name} execution time: {elapsed_time/nb_loops } seconds\")\n",
        "    print(f\"{model_name} max memory footprint: { max_memory*1e-9 } GB\")"
      ],
      "metadata": {
        "id": "9ZoyyqyEQbRP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
